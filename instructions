#1.获取网页内容
目的：向浏览器发送请求以返回内容
库：requests 作用是发送http请求
1.1发送请求
发送请求使用GET方法，一般http请求包括请求行，请求头与请求体
请求行包括方法类型（get）、资源路径（要采集的网页的后半截）和协议版本
请求头包括主机域名（host）（网页前半段）、user-agent（声明请求由谁发出）、accept（客户端想接收的响应数据类型，html/json）等
请求体一般放客户端传给服务器端其他任意数据，但是get方法的请求体一般是空的
1.2返回http响应
响应也由三个部分组成，状态行、相应头和响应体
状态行包括协议版本、状态码和状态消息
响应头包括生成响应的时间、内容类型及编码格式（utf-8）
响应体是服务器想给客户端的数据内容
网页信息包括：css（定义网页的样式）、html（定义网页的结构和信息）、javascript（定义用户和网页的交互逻辑）
剩下两个是前端开发要考虑的事情，与爬虫无关
#2.解析网页页面
目的：提取想要的数据
库：Beautiful Soup（bs4） 作用是解析获取到的html内容，提取信息
爬虫的核心在于html信息中提取到我们想要的信息
html内容里会包括body（文档的主体内容），body会包括h1（一级标题）、p（文本段落），
b加粗，i斜体，u加下划线，a添加链接
ol是有序列表，会自动加数字，里面的元素要用li标签，ul是无序列表
table是表格，thead一般表示表格的第一行，tbody表示表格主体，这两里面会用tr表示表格行，tr里面会用td表示表格中的数据
class作用是定义元素类名称
div和span是容器，一般要采集的数据很多都是它们的子元素，它们的区别就是div是块级元素，一般会独占一块，但span是内联元素，不会独占一块
br表示换行，img加图片，这两只有起始标签，没有闭合标签，（可以通过此来进行定位）
2.1
from bs4 import BeautifulSoup
import requests
content = requests.get("网页完整的url").text #将string存储在contenr中
soup = BeautifulSoup(content,"html.parser") #将内容解析成树状结构
all_prices = soup.findAll("p",attrs={"class":"price_color"}) #寻找共性属性
for price in all_prices:
  print(price.string)#可以在string后切片
all_titles = soup.findAll("h3")
for title in all_titles:
  all_links = title.findAll("a")
  for link in all_links:
    print(link.string)
2.2
完整带for循环的代码：
headers = {"User-Agent": "xxx"……}
for start_num in range(0,250，25)：#结束值不包含在范围里
 request.get("网页完整的url{start_num}",headers = headers)
 content = requests.get("网页完整的url").text #将string存储在contenr中
 soup = BeautifulSoup(content,"html.parser") #将内容解析成树状结构
 all_prices = soup.findAll("p",attrs={"class":"price_color"}) #寻找共性属性
 for price in all_prices:
  print(price.string)#可以在string后切片
#3.存储或分析数据
目的：存储我们要得到的数据
库：re 作用是通过正则表达式对采集到的数据进行预处理
